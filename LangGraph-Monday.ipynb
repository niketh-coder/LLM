{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11823964,"sourceType":"datasetVersion","datasetId":7427482}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Adding Summarization of the retrieved answers from llms using another llm","metadata":{"id":"E0oy6NB5mBRy"}},{"cell_type":"markdown","source":"## Installing dependencies","metadata":{"id":"-HYSCjy_mQeh"}},{"cell_type":"code","source":"with open(\"requirements.txt\", \"w\") as f:\n    f.write(\"\"\"chromadb\n                sentence-transformers\n                tree-sitter-languages\n                tree-sitter\n                tree-sitter-python\n                tree-sitter-javascript\n                langgraph\n            \"\"\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I6_puOpqmL56","outputId":"7ce5ac8b-b21f-4a73-cf96-dad6fed966d7","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:59:03.931613Z","iopub.execute_input":"2025-05-19T10:59:03.931791Z","iopub.status.idle":"2025-05-19T10:59:03.939075Z","shell.execute_reply.started":"2025-05-19T10:59:03.931774Z","shell.execute_reply":"2025-05-19T10:59:03.938355Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install -r /kaggle/working/requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:59:03.941040Z","iopub.execute_input":"2025-05-19T10:59:03.941289Z","iopub.status.idle":"2025-05-19T11:01:27.918435Z","shell.execute_reply.started":"2025-05-19T10:59:03.941272Z","shell.execute_reply":"2025-05-19T11:01:27.917695Z"}},"outputs":[{"name":"stdout","text":"Collecting chromadb (from -r /kaggle/working/requirements.txt (line 1))\n  Downloading chromadb-1.0.9-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 2)) (3.4.1)\nCollecting tree-sitter-languages (from -r /kaggle/working/requirements.txt (line 3))\n  Downloading tree_sitter_languages-1.10.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nCollecting tree-sitter (from -r /kaggle/working/requirements.txt (line 4))\n  Downloading tree_sitter-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.8 kB)\nCollecting tree-sitter-python (from -r /kaggle/working/requirements.txt (line 5))\n  Downloading tree_sitter_python-0.23.6-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\nCollecting tree-sitter-javascript (from -r /kaggle/working/requirements.txt (line 6))\n  Downloading tree_sitter_javascript-0.23.1-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\nCollecting langgraph (from -r /kaggle/working/requirements.txt (line 7))\n  Downloading langgraph-0.4.5-py3-none-any.whl.metadata (7.3 kB)\nCollecting build>=1.0.3 (from chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /kaggle/working/requirements.txt (line 1)) (2.11.4)\nCollecting fastapi==0.115.9 (from chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\nCollecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /kaggle/working/requirements.txt (line 1)) (1.26.4)\nCollecting posthog>=2.4.0 (from chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading posthog-4.0.1-py2.py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /kaggle/working/requirements.txt (line 1)) (4.13.2)\nCollecting onnxruntime>=1.14.1 (from chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /kaggle/working/requirements.txt (line 1)) (1.31.1)\nCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.33.1-py3-none-any.whl.metadata (2.5 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading opentelemetry_instrumentation_fastapi-0.54b1-py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /kaggle/working/requirements.txt (line 1)) (1.31.1)\nRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /kaggle/working/requirements.txt (line 1)) (0.21.1)\nCollecting pypika>=0.48.9 (from chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /kaggle/working/requirements.txt (line 1)) (4.67.1)\nRequirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /kaggle/working/requirements.txt (line 1)) (7.7.0)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /kaggle/working/requirements.txt (line 1)) (6.5.2)\nRequirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /kaggle/working/requirements.txt (line 1)) (1.72.0rc1)\nCollecting bcrypt>=4.0.1 (from chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /kaggle/working/requirements.txt (line 1)) (0.15.2)\nCollecting kubernetes>=28.1.0 (from chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /kaggle/working/requirements.txt (line 1)) (9.1.2)\nRequirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /kaggle/working/requirements.txt (line 1)) (6.0.2)\nCollecting mmh3>=4.0.1 (from chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\nRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /kaggle/working/requirements.txt (line 1)) (3.10.16)\nRequirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /kaggle/working/requirements.txt (line 1)) (0.28.1)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /kaggle/working/requirements.txt (line 1)) (14.0.0)\nRequirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb->-r /kaggle/working/requirements.txt (line 1)) (4.23.0)\nCollecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (4.51.3)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (1.15.2)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (0.31.1)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (11.1.0)\nRequirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph->-r /kaggle/working/requirements.txt (line 7)) (0.3.50)\nCollecting langgraph-checkpoint<3.0.0,>=2.0.26 (from langgraph->-r /kaggle/working/requirements.txt (line 7))\n  Downloading langgraph_checkpoint-2.0.26-py3-none-any.whl.metadata (4.6 kB)\nCollecting langgraph-prebuilt>=0.1.8 (from langgraph->-r /kaggle/working/requirements.txt (line 7))\n  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\nCollecting langgraph-sdk>=0.1.42 (from langgraph->-r /kaggle/working/requirements.txt (line 7))\n  Downloading langgraph_sdk-0.1.69-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph->-r /kaggle/working/requirements.txt (line 7)) (3.5.0)\nRequirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb->-r /kaggle/working/requirements.txt (line 1)) (25.0)\nCollecting pyproject_hooks (from build>=1.0.3->chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (4.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (2025.4.26)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (0.14.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (2025.3.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (2.32.3)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (1.1.0)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (0.24.0)\nRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (1.17.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (2.40.1)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (1.8.0)\nRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (2.0.0)\nRequirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (3.2.2)\nRequirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (2.4.0)\nCollecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph->-r /kaggle/working/requirements.txt (line 7)) (0.3.23)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph->-r /kaggle/working/requirements.txt (line 7)) (1.33)\nCollecting packaging>=19.1 (from build>=1.0.3->chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.26->langgraph->-r /kaggle/working/requirements.txt (line 7))\n  Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb->-r /kaggle/working/requirements.txt (line 1)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb->-r /kaggle/working/requirements.txt (line 1)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb->-r /kaggle/working/requirements.txt (line 1)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb->-r /kaggle/working/requirements.txt (line 1)) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb->-r /kaggle/working/requirements.txt (line 1)) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.5->chromadb->-r /kaggle/working/requirements.txt (line 1)) (2.4.1)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb->-r /kaggle/working/requirements.txt (line 1)) (25.2.10)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb->-r /kaggle/working/requirements.txt (line 1)) (3.20.3)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb->-r /kaggle/working/requirements.txt (line 1)) (1.13.1)\nRequirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (1.2.18)\nCollecting importlib-metadata<8.7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (1.70.0)\nCollecting opentelemetry-exporter-otlp-proto-common==1.33.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading opentelemetry_exporter_otlp_proto_common-1.33.1-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-proto==1.33.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading opentelemetry_proto-1.33.1-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-sdk>=1.2.0 (from chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading opentelemetry_sdk-1.33.1-py3-none-any.whl.metadata (1.6 kB)\nCollecting protobuf (from onnxruntime>=1.14.1->chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\nCollecting opentelemetry-instrumentation-asgi==0.54b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading opentelemetry_instrumentation_asgi-0.54b1-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.54b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading opentelemetry_instrumentation-0.54b1-py3-none-any.whl.metadata (6.8 kB)\nCollecting opentelemetry-semantic-conventions==0.54b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl.metadata (2.5 kB)\nCollecting opentelemetry-util-http==0.54b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading opentelemetry_util_http-0.54b1-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.54b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (1.17.2)\nCollecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.54b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading opentelemetry_api-1.33.1-py3-none-any.whl.metadata (1.6 kB)\nCollecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (1.9.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb->-r /kaggle/working/requirements.txt (line 1)) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb->-r /kaggle/working/requirements.txt (line 1)) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb->-r /kaggle/working/requirements.txt (line 1)) (0.4.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (2.19.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2))\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2))\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2))\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2))\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2))\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2))\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2))\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (3.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb->-r /kaggle/working/requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (2024.11.6)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (0.5.3)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (8.1.8)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (1.5.4)\nCollecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->-r /kaggle/working/requirements.txt (line 1)) (15.0.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (3.6.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (4.9.1)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (3.21.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph->-r /kaggle/working/requirements.txt (line 7)) (3.0.0)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph->-r /kaggle/working/requirements.txt (line 7)) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core>=0.1->langgraph->-r /kaggle/working/requirements.txt (line 7)) (0.23.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (0.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (3.4.2)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (1.3.1)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r /kaggle/working/requirements.txt (line 1))\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers->-r /kaggle/working/requirements.txt (line 2)) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.5->chromadb->-r /kaggle/working/requirements.txt (line 1)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.5->chromadb->-r /kaggle/working/requirements.txt (line 1)) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.5->chromadb->-r /kaggle/working/requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22.5->chromadb->-r /kaggle/working/requirements.txt (line 1)) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22.5->chromadb->-r /kaggle/working/requirements.txt (line 1)) (2024.2.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r /kaggle/working/requirements.txt (line 1)) (0.6.1)\nDownloading chromadb-1.0.9-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tree_sitter_languages-1.10.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tree_sitter-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (575 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.6/575.6 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tree_sitter_python-0.23.6-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (112 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tree_sitter_javascript-0.23.1-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langgraph-0.4.5-py3-none-any.whl (155 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\nDownloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langgraph_checkpoint-2.0.26-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\nDownloading langgraph_sdk-0.1.69-py3-none-any.whl (48 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.33.1-py3-none-any.whl (18 kB)\nDownloading opentelemetry_exporter_otlp_proto_common-1.33.1-py3-none-any.whl (18 kB)\nDownloading opentelemetry_proto-1.33.1-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.54b1-py3-none-any.whl (12 kB)\nDownloading opentelemetry_instrumentation-0.54b1-py3-none-any.whl (31 kB)\nDownloading opentelemetry_instrumentation_asgi-0.54b1-py3-none-any.whl (16 kB)\nDownloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl (194 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_api-1.33.1-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_util_http-0.54b1-py3-none-any.whl (7.3 kB)\nDownloading opentelemetry_sdk-1.33.1-py3-none-any.whl (118 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading posthog-4.0.1-py2.py3-none-any.whl (92 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading durationpy-0.10-py3-none-any.whl (3.9 kB)\nDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\nDownloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\nDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53801 sha256=733ead52e03a892540948a29b2ce6e69e5190fea55c3500931df6e9839c7c6b3\n  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\nSuccessfully built pypika\nInstalling collected packages: pypika, durationpy, uvloop, uvicorn, tree-sitter-python, tree-sitter-javascript, tree-sitter, python-dotenv, pyproject_hooks, protobuf, packaging, ormsgpack, opentelemetry-util-http, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, mmh3, importlib-metadata, humanfriendly, httptools, bcrypt, backoff, asgiref, watchfiles, tree-sitter-languages, starlette, posthog, opentelemetry-proto, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, build, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, nvidia-cusolver-cu12, langgraph-sdk, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langgraph-checkpoint, opentelemetry-instrumentation-fastapi, langgraph-prebuilt, langgraph, onnxruntime, chromadb\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: packaging\n    Found existing installation: packaging 25.0\n    Uninstalling packaging-25.0:\n      Successfully uninstalled packaging-25.0\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: importlib-metadata\n    Found existing installation: importlib_metadata 8.7.0\n    Uninstalling importlib_metadata-8.7.0:\n      Successfully uninstalled importlib_metadata-8.7.0\n  Attempting uninstall: opentelemetry-api\n    Found existing installation: opentelemetry-api 1.31.1\n    Uninstalling opentelemetry-api-1.31.1:\n      Successfully uninstalled opentelemetry-api-1.31.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: opentelemetry-semantic-conventions\n    Found existing installation: opentelemetry-semantic-conventions 0.52b1\n    Uninstalling opentelemetry-semantic-conventions-0.52b1:\n      Successfully uninstalled opentelemetry-semantic-conventions-0.52b1\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n  Attempting uninstall: opentelemetry-sdk\n    Found existing installation: opentelemetry-sdk 1.31.1\n    Uninstalling opentelemetry-sdk-1.31.1:\n      Successfully uninstalled opentelemetry-sdk-1.31.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-bigtable 2.30.0 requires google-api-core[grpc]<3.0.0,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chromadb-1.0.9 coloredlogs-15.0.1 durationpy-0.10 fastapi-0.115.9 httptools-0.6.4 humanfriendly-10.0 importlib-metadata-8.6.1 kubernetes-32.0.1 langgraph-0.4.5 langgraph-checkpoint-2.0.26 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.69 mmh3-5.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnxruntime-1.22.0 opentelemetry-api-1.33.1 opentelemetry-exporter-otlp-proto-common-1.33.1 opentelemetry-exporter-otlp-proto-grpc-1.33.1 opentelemetry-instrumentation-0.54b1 opentelemetry-instrumentation-asgi-0.54b1 opentelemetry-instrumentation-fastapi-0.54b1 opentelemetry-proto-1.33.1 opentelemetry-sdk-1.33.1 opentelemetry-semantic-conventions-0.54b1 opentelemetry-util-http-0.54b1 ormsgpack-1.9.1 packaging-24.2 posthog-4.0.1 protobuf-5.29.4 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.1.0 starlette-0.45.3 tree-sitter-0.24.0 tree-sitter-javascript-0.23.1 tree-sitter-languages-1.10.2 tree-sitter-python-0.23.6 uvicorn-0.34.2 uvloop-0.21.0 watchfiles-1.0.5\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Importing Codebase from Drive","metadata":{"id":"n_iplMIloFB5"}},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')\n!git clone https://github.com/Healthlane-Technologies/Zango.git\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h9jYLhUioKe5","outputId":"e07e78cd-44ee-4f3e-c5a5-9cc2ab557ef3","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:01:27.919327Z","iopub.execute_input":"2025-05-19T11:01:27.919607Z","iopub.status.idle":"2025-05-19T11:01:36.726872Z","shell.execute_reply.started":"2025-05-19T11:01:27.919581Z","shell.execute_reply":"2025-05-19T11:01:36.726186Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'Zango'...\nremote: Enumerating objects: 22087, done.\u001b[K\nremote: Counting objects: 100% (2700/2700), done.\u001b[K\nremote: Compressing objects: 100% (886/886), done.\u001b[K\nremote: Total 22087 (delta 2148), reused 1928 (delta 1773), pack-reused 19387 (from 2)\u001b[K\nReceiving objects: 100% (22087/22087), 159.90 MiB | 39.20 MiB/s, done.\nResolving deltas: 100% (11320/11320), done.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"XOcXV_oJl86T"}},{"cell_type":"code","source":"import os\nimport json\nimport re\nimport time\nimport ast\nimport uuid\nimport numpy as np\nfrom pathlib import Path\nimport requests\nfrom typing import TypedDict, Optional, List, Annotated, Dict, Any\nfrom chromadb import PersistentClient\nfrom chromadb.config import Settings\nfrom chromadb.utils.batch_utils import create_batches\nimport tree_sitter_python\nimport tree_sitter_javascript\nfrom tree_sitter import Language, Parser\nimport subprocess\nfrom sentence_transformers import SentenceTransformer, CrossEncoder\nfrom transformers import T5ForConditionalGeneration, AutoTokenizer\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom kaggle_secrets import UserSecretsClient\n# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM","metadata":{"id":"Oepn2EVFlxpt","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:01:36.727908Z","iopub.execute_input":"2025-05-19T11:01:36.728267Z","iopub.status.idle":"2025-05-19T11:02:16.510868Z","shell.execute_reply.started":"2025-05-19T11:01:36.728226Z","shell.execute_reply":"2025-05-19T11:02:16.510213Z"}},"outputs":[{"name":"stderr","text":"2025-05-19 11:01:57.044124: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747652517.479993      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747652517.600555      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Defining Variables","metadata":{"id":"YZEuTQnVmnjR"}},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nmemory = MemorySaver()\n\nBASE_CODE_PATH = \"/kaggle/working/Zango\"\nCHUNK_SIZE = 300\nEMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\nSUMMARIZATION_MODEL = \"all-MiniLM-L6-v2\"\nMAX_RECENT_MESSAGES = 5\nGROQ_MODEL = \"llama3-70b-8192\"\nGROQ_API_KEY = user_secrets.get_secret('GROQ_API')","metadata":{"id":"F_GEPq-7mqjg","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:02:16.511685Z","iopub.execute_input":"2025-05-19T11:02:16.512250Z","iopub.status.idle":"2025-05-19T11:02:16.628150Z","shell.execute_reply.started":"2025-05-19T11:02:16.512220Z","shell.execute_reply":"2025-05-19T11:02:16.627384Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Define Chunker","metadata":{"id":"TewfygKfnT_4"}},{"cell_type":"code","source":"class CodeChunker:\n    def __init__(self):\n        self.lang_map = {\n            '.py': 'python',\n            '.js': 'javascript',\n            '.jsx': 'javascript',\n            '.ts': 'typescript',\n            '.tsx': 'typescript',\n        }\n        self.parsers = self._initialize_parsers()\n\n    def _initialize_parsers(self):\n        \"\"\"Initialize parsers with proper error handling\"\"\"\n        parsers = {}\n        supported_languages = {'python', 'javascript', 'typescript'}\n        \n        # Import the tree-sitter module only if it's being used\n        try:\n            from tree_sitter import Parser\n            \n            # Attempt to load language libraries\n            for lang in supported_languages:\n                try:\n                    # This assumes you have the language libraries installed\n                    from tree_sitter import Language\n                    # Get the path to the language library\n                    # This is a common pattern, but you might need to adjust paths\n                    LANGUAGE_PATH = f\"./tree-sitter-{lang}.so\"\n                    if os.path.exists(LANGUAGE_PATH):\n                        language = Language(LANGUAGE_PATH, lang)\n                        parser = Parser()\n                        parser.set_language(language)\n                        parsers[lang] = parser\n                    else:\n                        print(f\"Language library not found for {lang} at {LANGUAGE_PATH}\")\n                except Exception as e:\n                    print(f\"Error setting up parser for {lang}: {e}\")\n        except ImportError:\n            print(\"tree-sitter module not found. Falling back to regex-based chunking.\")\n        \n        return parsers\n\n    def _ast_chunk_python(self, code):\n        \"\"\"AST-based chunking for Python using built-in ast module\"\"\"\n        try:\n            tree = ast.parse(code)\n            chunks = []\n            \n            # Extract imports for context\n            imports = []\n            for node in ast.walk(tree):\n                if isinstance(node, (ast.Import, ast.ImportFrom)):\n                    import_lineno = node.lineno\n                    import_end_lineno = getattr(node, 'end_lineno', import_lineno)\n                    lines = code.split('\\n')\n                    import_text = '\\n'.join(lines[import_lineno-1:import_end_lineno])\n                    imports.append(import_text)\n            \n            # Process functions and classes\n            for node in tree.body:\n                if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                    # Get the source lines for this node\n                    start_line = node.lineno\n                    end_line = node.end_lineno if hasattr(node, 'end_lineno') else start_line\n                    \n                    # Extract the chunk text\n                    lines = code.split('\\n')\n                    chunk_text = '\\n'.join(lines[start_line-1:end_line])\n                    \n                    # Create chunk with metadata\n                    node_type = 'class' if isinstance(node, ast.ClassDef) else 'function'\n                    if isinstance(node, ast.AsyncFunctionDef):\n                        node_type = 'async_function'\n                        \n                    chunks.append({\n                        'code': chunk_text,\n                        'type': node_type,\n                        'name': node.name,\n                        'start_line': start_line,\n                        'end_line': end_line\n                    })\n                    \n                    # Process nested functions and classes within classes\n                    if isinstance(node, ast.ClassDef):\n                        for child_node in node.body:\n                            if isinstance(child_node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n                                child_start_line = child_node.lineno\n                                child_end_line = child_node.end_lineno if hasattr(child_node, 'end_lineno') else child_start_line\n                                \n                                child_text = '\\n'.join(lines[child_start_line-1:child_end_line])\n                                \n                                child_type = 'method'\n                                if isinstance(child_node, ast.AsyncFunctionDef):\n                                    child_type = 'async_method'\n                                \n                                chunks.append({\n                                    'code': child_text,\n                                    'type': child_type,\n                                    'name': child_node.name,\n                                    'parent': node.name,\n                                    'start_line': child_start_line,\n                                    'end_line': child_end_line\n                                })\n            \n            # Add context to chunks\n            context = {'imports': imports}\n            for chunk in chunks:\n                chunk['context'] = context\n            \n            return chunks or [self._create_full_file_chunk(code)]\n        except SyntaxError:\n            # Fall back to regex if AST parsing fails\n            return self._regex_chunk(code, 'python')\n\n    def _regex_chunk_javascript(self, code, lang):\n        \"\"\"Regex-based chunking for JavaScript/TypeScript\"\"\"\n        patterns = {\n            'function': r'(function)\\s+(\\w+)\\s*\\([^)]*\\)\\s*\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}',\n            'arrow_function': r'(const|let|var)\\s+(\\w+)\\s*=\\s*(?:\\([^)]*\\)|[^=]*)\\s*=>\\s*\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}',\n            'class': r'(class)\\s+(\\w+)(?:\\s+extends\\s+\\w+)?\\s*\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}',\n            'method': r'(\\w+)\\s*\\([^)]*\\)\\s*\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}',\n            'component': r'(const|function)\\s+(\\w+)\\s*=?\\s*(?:\\([^)]*\\))?\\s*=?\\s*(?:=>)?\\s*\\{\\s*(?:.*?return\\s*\\(?\\s*<.*?>.*?<\\/.*?>\\s*\\)?)?(?:[^{}]|(?:\\{[^{}]*\\}))*\\}'\n        }\n        \n        if lang in ['typescript', 'tsx']:\n            patterns.update({\n                'interface': r'(interface)\\s+(\\w+)(?:\\s+extends\\s+\\w+)?\\s*\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}',\n                'type': r'(type)\\s+(\\w+)\\s*=\\s*(?:[^;]|(?:\\{[^{}]*\\}))*;'\n            })\n        \n        chunks = []\n        \n        # Extract imports for context\n        imports = []\n        import_pattern = r'import\\s+(?:{[^}]+}|[^;]+)\\s+from\\s+[\\'\"]([^\\'\"]+)[\\'\"];?'\n        for match in re.finditer(import_pattern, code, re.MULTILINE):\n            import_text = code[match.start():match.end()]\n            imports.append(import_text)\n        \n        # Extract exports for context\n        exports = []\n        export_pattern = r'export\\s+(?:default\\s+)?(?:function|class|const|let|var)?\\s*(\\w+)'\n        for match in re.finditer(export_pattern, code, re.MULTILINE):\n            export_text = code[match.start():match.end()]\n            exports.append(export_text)\n        \n        # Process each pattern\n        for chunk_type, pattern in patterns.items():\n            for match in re.finditer(pattern, code, re.DOTALL):\n                # Try to extract name from capture group\n                name = match.group(2) if len(match.groups()) > 1 else None\n                \n                # For methods, which don't have a type identifier in the first group\n                if chunk_type == 'method' and not name:\n                    name = match.group(1)\n                \n                chunks.append({\n                    'code': match.group(0),\n                    'type': chunk_type,\n                    'name': name,\n                    'start_line': code[:match.start()].count('\\n') + 1,\n                    'end_line': code[:match.end()].count('\\n') + 1,\n                    'context': {\n                        'imports': imports,\n                        'exports': exports\n                    }\n                })\n        \n        # Sort chunks by start position\n        chunks.sort(key=lambda x: x['start_line'])\n        \n        return chunks or [self._create_full_file_chunk(code)]\n\n    def _regex_chunk(self, code, lang):\n        \"\"\"Regex fallback chunking with language-specific patterns\"\"\"\n        if lang == 'python':\n            patterns = {\n                'function': r'(def)\\s+(\\w+)\\s*\\([^)]*\\)(?:\\s*->.*?)?:\\s*(?:\\r?\\n\\s+.+)*(?:\\r?\\n)*',\n                'class': r'(class)\\s+(\\w+).*?(?:\\r?\\n(?:\\s+[^\\r\\n]*)?)*',\n                'import': r'(from|import)\\s+.*?(?:\\r?\\n)?'\n            }\n        else:  # For non-Python languages, use the JavaScript/TypeScript chunker\n            return self._regex_chunk_javascript(code, lang)\n        \n        chunks = []\n        \n        for chunk_type, pattern in patterns.items():\n            for match in re.finditer(pattern, code, re.DOTALL):\n                # Try to extract name from capture group\n                name = match.group(2) if len(match.groups()) > 1 else None\n                \n                chunks.append({\n                    'code': match.group(0),\n                    'type': chunk_type,\n                    'name': name,\n                    'start_line': code[:match.start()].count('\\n') + 1,\n                    'end_line': code[:match.end()].count('\\n') + 1\n                })\n        \n        # Sort chunks by start position\n        chunks.sort(key=lambda x: x['start_line'])\n        \n        return chunks or [self._create_full_file_chunk(code)]\n\n    def _create_full_file_chunk(self, code):\n        \"\"\"Create a full-file chunk when no sub-chunks found\"\"\"\n        return {\n            'code': code,\n            'type': 'file',\n            'name': None,\n            'start_line': 1,\n            'end_line': code.count('\\n') + 1\n        }\n\n    def _extract_node_name(self, node, code):\n        \"\"\"Extract the name of a function, class, etc. from its node\"\"\"\n        # Look for identifier child node\n        for child in node.children:\n            if child.type == 'identifier':\n                return code[child.start_byte:child.end_byte]\n            \n        # For JSX/TSX components\n        if node.type in ['jsx_element', 'tsx_element']:\n            for child in node.children:\n                if child.type in ['jsx_opening_element', 'tsx_opening_element']:\n                    for grandchild in child.children:\n                        if grandchild.type in ['identifier', 'nested_identifier']:\n                            return code[grandchild.start_byte:grandchild.end_byte]\n        \n        return None\n\n    def _process_react_component(self, code, lang):\n        \"\"\"Extract React component details\"\"\"\n        component_info = {\n            'props': [],\n            'state': [],\n            'hooks': [],\n            'jsx_elements': []\n        }\n        \n        # Extract props from function parameters or class props\n        props_pattern = r'(?:function|const)\\s+\\w+\\s*\\(\\s*(?:{([^}]+)}|\\s*(\\w+)[^)]*)\\s*\\)'\n        props_match = re.search(props_pattern, code)\n        if props_match:\n            props_str = props_match.group(1) or props_match.group(2)\n            if props_str:\n                component_info['props'] = [p.strip() for p in props_str.split(',')]\n        \n        # Extract useState hooks\n        state_pattern = r'const\\s+\\[(\\w+),\\s*set(\\w+)\\]\\s*=\\s*useState'\n        for match in re.finditer(state_pattern, code, re.IGNORECASE):\n            component_info['state'].append(match.group(1))\n        \n        # Extract other hooks\n        hooks_pattern = r'use(\\w+)\\('\n        for match in re.finditer(hooks_pattern, code):\n            if match.group(0) not in ['useState(']:\n                component_info['hooks'].append(match.group(0))\n        \n        # Extract JSX elements (simplified)\n        jsx_pattern = r'<(\\w+)[^>]*>'\n        for match in re.finditer(jsx_pattern, code):\n            if match.group(1)[0].isupper():  # Component names start with uppercase\n                component_info['jsx_elements'].append(match.group(1))\n        \n        return component_info\n\n    def _process_django_template(self, code):\n        \"\"\"Extract Django template details\"\"\"\n        template_info = {\n            'blocks': [],\n            'includes': [],\n            'extends': None,\n            'tags': []\n        }\n        \n        # Extract template inheritance\n        extends_match = re.search(r'{%\\s*extends\\s+[\\'\"]([^\\'\"]+)[\\'\"]', code)\n        if extends_match:\n            template_info['extends'] = extends_match.group(1)\n        \n        # Extract blocks\n        block_pattern = r'{%\\s*block\\s+(\\w+)[^%]*%}'\n        for match in re.finditer(block_pattern, code):\n            template_info['blocks'].append(match.group(1))\n        \n        # Extract includes\n        include_pattern = r'{%\\s*include\\s+[\\'\"]([^\\'\"]+)[\\'\"]'\n        for match in re.finditer(include_pattern, code):\n            template_info['includes'].append(match.group(1))\n        \n        # Extract custom tags\n        tag_pattern = r'{%\\s*(\\w+)\\s'\n        for match in re.finditer(tag_pattern, code):\n            tag = match.group(1)\n            if tag not in ['block', 'endblock', 'if', 'endif', 'for', 'endfor', 'include', 'extends']:\n                template_info['tags'].append(tag)\n        \n        return template_info\n\n    def _extract_context(self, code, lang):\n        \"\"\"Extract imports and other context information\"\"\"\n        context = {\n            'imports': [],\n            'exports': [],\n            'dependencies': []\n        }\n        \n        # Extract imports based on language\n        if lang == 'python':\n            import_pattern = r'^(?:from\\s+(\\S+)\\s+import\\s+(.+)|import\\s+(.+))$'\n            for match in re.finditer(import_pattern, code, re.MULTILINE):\n                if match.group(1):  # from X import Y\n                    module = match.group(1)\n                    imports = match.group(2)\n                    context['imports'].append(f\"from {module} import {imports}\")\n                    context['dependencies'].append(module)\n                else:  # import X\n                    imports = match.group(3)\n                    context['imports'].append(f\"import {imports}\")\n                    context['dependencies'].append(imports.split('.')[0])\n        \n        elif lang in ['javascript', 'typescript']:\n            # ES6 imports\n            import_pattern = r'import\\s+(?:{([^}]+)}|([^;]+))\\s+from\\s+[\\'\"]([^\\'\"]+)[\\'\"]'\n            for match in re.finditer(import_pattern, code, re.MULTILINE):\n                named_imports = match.group(1)\n                default_import = match.group(2)\n                module = match.group(3)\n                \n                if named_imports:\n                    context['imports'].append(f\"import {{ {named_imports} }} from '{module}'\")\n                else:\n                    context['imports'].append(f\"import {default_import} from '{module}'\")\n                    \n                context['dependencies'].append(module)\n                \n            # ES6 exports\n            export_pattern = r'export\\s+(?:default\\s+)?(?:function|class|const|let|var)?\\s*(\\w+)'\n            for match in re.finditer(export_pattern, code, re.MULTILINE):\n                context['exports'].append(match.group(1))\n        \n        return context\n\n    def chunk_code(self, code, lang):\n        \"\"\"Main chunking method with fallback logic and context preservation\"\"\"\n        # Extract context information first\n        context = self._extract_context(code, lang)\n        \n        # Use AST for Python, regex for JavaScript/TypeScript\n        if lang == 'python':\n            chunks = self._ast_chunk_python(code)\n        else:\n            chunks = self._regex_chunk_javascript(code, lang)\n        \n        # Add context to each chunk if not already present\n        for chunk in chunks:\n            if 'context' not in chunk:\n                chunk['context'] = context\n        \n        return chunks\n\n    def process_directory(self, base_path):\n        \"\"\"Process directory with enhanced error handling and metadata\"\"\"\n        chunks = []\n        file_count = 0\n        processed_count = 0\n        \n        for root, _, files in os.walk(base_path):\n            for file in files:\n                path = Path(root) / file\n                if path.suffix not in self.lang_map:\n                    continue\n                    \n                file_count += 1\n                try:\n                    with open(path, 'r', encoding='utf-8') as f:\n                        code = f.read()\n                except UnicodeDecodeError:\n                    print(f\"Skipping non-text file: {path}\")\n                    continue\n                except Exception as e:\n                    print(f\"Error reading {path}: {e}\")\n                    continue\n                    \n                processed_count += 1\n                lang = self.lang_map[path.suffix]\n                \n                # Special handling for Django templates\n                if path.suffix == '.html' and ('{% ' in code or '{{ ' in code):\n                    template_info = self._process_django_template(code)\n                else:\n                    template_info = None\n                    \n                for chunk in self.chunk_code(code, lang):\n                    relative_path = str(path.relative_to(base_path))\n                    chunk_data = {\n                        **chunk,\n                        'file': relative_path,\n                        'language': lang,\n                        'file_size': len(code),\n                        'path_parts': relative_path.split('/'),\n                    }\n                    \n                    if template_info:\n                        chunk_data['template_info'] = template_info\n                        \n                    chunks.append(chunk_data)\n                    \n        print(f\"Processed {processed_count}/{file_count} files, extracted {len(chunks)} chunks\")\n        return chunks\n","metadata":{"id":"THg4NXtdnPf7","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:02:16.630279Z","iopub.execute_input":"2025-05-19T11:02:16.630936Z","iopub.status.idle":"2025-05-19T11:02:16.666084Z","shell.execute_reply.started":"2025-05-19T11:02:16.630916Z","shell.execute_reply":"2025-05-19T11:02:16.665353Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Define Vector storage","metadata":{"id":"i7wiDsffnbFJ"}},{"cell_type":"code","source":"chunker = CodeChunker()\nchunks = chunker.process_directory(BASE_CODE_PATH)\nwith open(\"code_chunks.json\", \"w\") as f:\n    json.dump(chunks, f, indent=2)\nprint(f\"Generated {len(chunks)} chunks in code_chunks.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:02:16.666854Z","iopub.execute_input":"2025-05-19T11:02:16.667053Z","iopub.status.idle":"2025-05-19T11:06:07.575492Z","shell.execute_reply.started":"2025-05-19T11:02:16.667038Z","shell.execute_reply":"2025-05-19T11:06:07.574722Z"}},"outputs":[{"name":"stdout","text":"Language library not found for javascript at ./tree-sitter-javascript.so\nLanguage library not found for typescript at ./tree-sitter-typescript.so\nLanguage library not found for python at ./tree-sitter-python.so\nProcessed 548/548 files, extracted 12422 chunks\nGenerated 12422 chunks in code_chunks.json\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"client = PersistentClient(path=\"./chroma_db\")\ncollection = client.get_or_create_collection(\n    name=\"codebase\",\n    metadata={\"hnsw:space\": \"cosine\"}\n)\nembedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\nwith open(\"/kaggle/working/code_chunks.json\") as f:\n    chunks = json.load(f)\n\ndef validate_metadata(chunk):\n    return {k: str(v) for k,v in chunk.items() \n            if k != \"code\" and isinstance(v, (str, int, float, bool))}\n\n# Prepare all data first\nids = [str(hash(chunk[\"code\"])) for chunk in chunks]  # Simple hashing\ndocuments = [chunk[\"code\"] for chunk in chunks]\nmetadatas = [validate_metadata(chunk) for chunk in chunks]\nembeddings = embedder.encode(\n    documents,\n    batch_size=64,\n    normalize_embeddings=True\n).astype(np.float32).tolist()\n\n# Create optimized batches\nbatches = create_batches(\n    api=client,\n    ids=ids,\n    documents=documents,\n    embeddings=embeddings,\n    metadatas=metadatas\n)\n\n# Insert with error handling\nfor i, batch in enumerate(batches):\n    try:\n        collection.add(**batch)\n        print(f\"Inserted batch {i+1}/{len(batches)}\")\n    except Exception as e:\n        print(f\"Failed batch {i}: {str(e)}\")\n        # Implement retry logic here\n\nprint(f\"Successfully stored {len(chunks)} chunks\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:06:07.576381Z","iopub.execute_input":"2025-05-19T11:06:07.576689Z","iopub.status.idle":"2025-05-19T11:06:31.579416Z","shell.execute_reply.started":"2025-05-19T11:06:07.576663Z","shell.execute_reply":"2025-05-19T11:06:31.578590Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41b78f6ddf844041bccb2616eb4f043b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"705be6c3012b4b45ab951974709bab46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9fc6092a08b45ae8866a0a89919fdcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"381979f96e914afbac3c343d2b82540d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a2dd2652f794b6fb6095c53b59aa55d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bfd6aeac0df47ed960d7e215b80b910"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dd5914562444a0a8592478bc6596c20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09f22295592c4f50984b44da909eadac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"405e237c208348a9a7ac676e601e88b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f817d5a1a76040c693929b484966128f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3dff2fc588a46b99464bc08df900506"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/195 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76a15ae41a9a416f9fc7d94a47d0085f"}},"metadata":{}},{"name":"stdout","text":"Failed batch 0: chromadb.api.models.Collection.Collection.add() argument after ** must be a mapping, not tuple\nFailed batch 1: chromadb.api.models.Collection.Collection.add() argument after ** must be a mapping, not tuple\nFailed batch 2: chromadb.api.models.Collection.Collection.add() argument after ** must be a mapping, not tuple\nSuccessfully stored 12422 chunks\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## LangGraph","metadata":{"id":"3sBfGQUTnqcF"}},{"cell_type":"markdown","source":"### Define Node","metadata":{"id":"3vHZ4jlDnsy-"}},{"cell_type":"code","source":"class ChatState(TypedDict):\n    question: str\n    processed_question: str\n    context: List[str]\n    answers: List[str]\n    final_answer: str\n    messages: List[str]\n    history_summary: str","metadata":{"id":"pxZKI4IfnpxF","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:06:31.580250Z","iopub.execute_input":"2025-05-19T11:06:31.580508Z","iopub.status.idle":"2025-05-19T11:06:31.584250Z","shell.execute_reply.started":"2025-05-19T11:06:31.580482Z","shell.execute_reply":"2025-05-19T11:06:31.583614Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### Define Workflow","metadata":{"id":"2GAIKwO7n1LC"}},{"cell_type":"code","source":"def create_workflow():\n    builder = StateGraph(ChatState)\n\n    def preprocess_with_llama(state): \n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an expert at improving search queries for a large language model to executie it properly. Your task is to enhance queries by adding synonyms for key terms and rephrasing short queries to add more context. Return ONLY the enhanced query without explanations and make it meaningfull.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Enhance this query: {state['question']}\"\n            }\n        ]\n        \n        # Call the LLaMA model via Groq 'Client' object has no attribute 'chat'\n        try:\n            response = requests.post(\n                \"https://api.groq.com/openai/v1/chat/completions\",\n                headers={\"Authorization\": f\"Bearer {GROQ_API_KEY}\"},\n                json={\n                    \"model\": GROQ_MODEL,\n                    \"messages\": messages,\n                    \"temperature\": 0.3,\n                    \"max_tokens\": 1000\n                }\n            )\n            response_data = response.json()\n\n            if \"choices\" in response_data and len(response_data[\"choices\"]) > 0:\n                enhanced_query = response_data[\"choices\"][0]['message'][\"content\"].strip()\n                state[\"processed_question\"] = enhanced_query\n            else:\n                print(\"Unexpected Format:\", {response_data})\n                state[\"processed_question\"] = state[\"question\"]\n            \n        except Exception as e:\n            # Fallback to original query if there's an error\n            print(f\"Error in preprocessing: {e}\")\n            state[\"processed_question\"] = state[\"question\"]\n        \n        return state\n    \n    def retrieve(state):\n        query = state.get(\"processed_question\", state[\"question\"])\n        embedding = SentenceTransformer(\"all-MiniLM-L6-v2\").encode(query)\n        results = collection.query(\n            query_embeddings=[embedding.tolist()],\n            n_results=10,\n            include=[\"documents\", \"metadatas\"]\n        )\n        return {\"context\": results[\"documents\"][0]}\n\n    def rank_and_filter_retrievals(state):\n        \n        retrieved_docs = state[\"context\"]\n        query = state.get(\"processed_question\", state[\"question\"])\n        \n        # Skip if there are no documents or only one document\n        if not retrieved_docs or len(retrieved_docs) <= 1:\n            return state\n        \n        # Use a lightweight cross-encoder for efficiency\n        cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-2-v2')\n        \n        # Score documents in batches for efficiency\n        batch_size = 16\n        all_scores = []\n        \n        for i in range(0, len(retrieved_docs), batch_size):\n            batch_docs = retrieved_docs[i:i+batch_size]\n            pairs = [[query, doc] for doc in batch_docs]\n            batch_scores = cross_encoder.predict(pairs)\n            all_scores.extend(batch_scores)\n        \n        # Normalize scores\n        scores = np.array(all_scores)\n        if len(scores) > 0:\n            min_score = scores.min()\n            max_score = scores.max()\n            if max_score > min_score:\n                normalized_scores = (scores - min_score) / (max_score - min_score)\n            else:\n                normalized_scores = np.ones_like(scores) * 0.5\n        else:\n            normalized_scores = np.array([])\n        \n        # Create scored documents\n        scored_docs = list(zip(normalized_scores, retrieved_docs))\n        \n        # Sort by score\n        scored_docs.sort(reverse=True)\n        \n        # Apply adaptive thresholding\n        if len(scored_docs) > 0:\n            max_score = scored_docs[0][0]\n            threshold = max(0.3, max_score * 0.7)  # Dynamic threshold based on top score\n            filtered_docs = [doc for score, doc in scored_docs if score >= threshold]\n            \n            # Ensure we have at least some documents\n            if not filtered_docs and scored_docs:\n                filtered_docs = [scored_docs[0][1]]\n        else:\n            filtered_docs = []\n        \n        # Limit to reasonable number of documents\n        max_docs = 5\n        top_docs = filtered_docs[:max_docs]\n        \n        state[\"context\"] = top_docs\n        return state\n\n\n    def identify_missing_functions(state):\n        \"\"\"Analyze retrieved code chunks and identify potentially missing functions\"\"\"\n        \n        # Create a prompt that asks the LLM to identify missing functions\n        response = requests.post(\n            \"https://api.groq.com/openai/v1/chat/completions\",\n            headers={\"Authorization\": f\"Bearer {GROQ_API_KEY}\"},\n            json={\n                \"model\": GROQ_MODEL,\n                \"messages\": [{\n                    \"role\": \"system\",\n                    \"content\": (\n                        \"You are a code analyzer. Your task is to examine code snippets and identify any functions or dependencies \"\n                        \"that are referenced but not defined in the provided code. Focus on identifying what additional code would be \"\n                        \"needed to fully understand and solve the user's problem.\"\n                    )\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": (\n                        f\"User Question: {state['question']}\\n\\n\"\n                        f\"Retrieved Code Snippets:\\n{state['context']}\\n\\n\"\n                        \"Please analyze these code snippets and identify any functions, classes, or dependencies that are referenced \"\n                        \"but not defined in the provided code. List the names of these missing components and briefly explain why \"\n                        \"they might be important for understanding the user's question.\"\n                    )\n                }]\n            }\n        ).json()\n        \n        missing_functions_analysis = response[\"choices\"][0][\"message\"][\"content\"]\n        \n        return {\"missing_functions_analysis\": missing_functions_analysis}\n\n    def retrieve_missing_functions(state):\n        \"\"\"Retrieve additional code for missing functions identified in the analysis\"\"\"\n        \n        # Skip if no missing functions were identified\n        if not state.get(\"missing_functions_analysis\") or \"No missing functions\" in state[\"missing_functions_analysis\"]:\n            return {\"additional_context\": []}\n        \n        # Extract function names from the analysis (simplified approach)\n        # In a real implementation, you might want to use regex or more sophisticated parsing\n        response = requests.post(\n            \"https://api.groq.com/openai/v1/chat/completions\",\n            headers={\"Authorization\": f\"Bearer {GROQ_API_KEY}\"},\n            json={\n                \"model\": GROQ_MODEL,\n                \"messages\": [{\n                    \"role\": \"system\",\n                    \"content\": \"Extract the names of missing functions from the analysis.\"\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"From this analysis:\\n{state['missing_functions_analysis']}\\n\\nExtract just the names of the missing functions or components as a comma-separated list.\"\n                }]\n            }\n        ).json()\n        \n        function_names = response[\"choices\"][0][\"message\"][\"content\"].split(\",\")\n        function_names = [name.strip() for name in function_names]\n        \n        # Retrieve additional code for each identified function\n        additional_context = []\n        for function_name in function_names:\n            if function_name:\n                # Use embedding search to find the function definition\n                # This assumes you have a similar collection setup as in your retrieve function\n                embedding = SentenceTransformer(\"all-MiniLM-L6-v2\").encode(function_name)\n                results = collection.query(\n                    query_embeddings=[embedding.tolist()],\n                    n_results=3,\n                    include=[\"documents\", \"metadatas\"]\n                )\n                \n                if results and \"documents\" in results and results[\"documents\"]:\n                    additional_context.extend(results[\"documents\"][0])\n        \n        return {\"additional_context\": additional_context}\n\n    \n    def manage_conversation_history(state):\n        \"\"\"Manage the conversation history by keeping recent messages and summarizing older ones\"\"\"\n        # Add current question to the messages\n        state[\"messages\"].append({\"role\": \"user\", \"content\": state[\"question\"]})\n        \n        if len(state[\"messages\"]) > 2 * MAX_RECENT_MESSAGES:\n            # Separate older messages to summarize\n            messages_to_summarize = state[\"messages\"][:-MAX_RECENT_MESSAGES]\n            recent_messages = state[\"messages\"][-MAX_RECENT_MESSAGES:]\n            \n            # If there's already a summary, include it for context\n            summary_prompt = \"\"\n            if state[\"history_summary\"]:\n                summary_prompt = f\"Previous conversation summary: {state['history_summary']}\\n\\n\"\n            \n            # Add messages to summarize\n            message_text = \"\"\n            for msg in messages_to_summarize:\n                prefix = \"User: \" if msg[\"role\"] == \"user\" else \"Assistant: \"\n                message_text += f\"{prefix}{msg['content']}\\n\"\n            \n            # Generate new summary of older conversations\n            response = requests.post(\n                \"https://api.groq.com/openai/v1/chat/completions\",\n                headers={\"Authorization\": f\"Bearer {GROQ_API_KEY}\"},\n                json={\n                    \"model\": GROQ_MODEL,\n                    \"messages\": [{\n                        \"role\": \"system\",\n                        \"content\": (\n                            \"You are a conversation summarizer. Condense the following conversation into a brief summary \"\n                            \"that captures the key points discussed and questions asked. Focus on the essential information \"\n                            \"that would be needed for context in future conversation.\"\n                        )\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": (\n                            f\"{summary_prompt}Please summarize this conversation fragment:\\n\\n{message_text}\"\n                        )\n                    }]\n                }\n            ).json()\n            \n            # Update the state with the summary and reduced message list\n            state[\"history_summary\"] = response[\"choices\"][0][\"message\"][\"content\"]\n            state[\"messages\"] = recent_messages\n        \n        return state\n    \n    def generate_answers(state):\n        # First manage conversation history\n        state = manage_conversation_history(state)\n        \n        # Create messages with context including conversation history\n        system_content = f\"Relevant code: {state['context']}\"\n        \n        # Add conversation history summary if it exists\n        if state.get(\"history_summary\"):\n            system_content = f\"Previous conversation summary: {state['history_summary']}\\n\\n{system_content}\"\n        \n        # Combine the most recent messages and system content\n        messages = [{\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in state[\"messages\"]]\n        messages.append({\"role\": \"system\", \"content\": system_content})\n        \n        answers = []\n        for code in state[\"context\"]:\n            response = requests.post(\n                \"https://api.groq.com/openai/v1/chat/completions\",\n                headers={\"Authorization\": f\"Bearer {GROQ_API_KEY}\"},\n                json={\n                    \"model\": GROQ_MODEL,\n                    \"messages\": [{\n                        \"role\": \"system\",\n                        \"content\": (\n                            \"You are a helpful and friendly support assistant for an internal company software. \"\n                            \"You know how the system works, including its features, permissions, and typical issues users may face. \"\n                            \"Your job is to explain possible reasons for the issue and guide the user to a solution in plain, simple language. \"\n                            \"Avoid technical terms, error codes, or developer jargon. Be clear, supportive, and patient.\\n\\n\"\n                            \"If the issue might be related to permissions, login state, or a missing step, gently suggest those as things to check. \"\n                            \"Only refer to code if it directly explains behavior, and describe it in user-friendly language.\\n\\n\"\n                            \"Your answers should be formatted clearly with bullet points or steps if needed.\\n\"\n                        )\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": (\n                            f\"A user is facing an issue with the software.\\n\\n\"\n                            f\"Previous conversation context: {state.get('history_summary', 'No prior context')}\\n\\n\"\n                            f\"Here is part of the codebase that may relate to the issue:\\n{code}\\n\\n\"\n                            f\"Recent messages: {state['messages'][-MAX_RECENT_MESSAGES:]}\\n\\n\"\n                            f\"User's Question: {state['question']}\\n\\n\"\n                            \"Please provide a clear, non-technical explanation and steps they can follow to solve the issue.\"\n                        )\n                    }]\n                }\n            ).json()\n            answers.append(response[\"choices\"][0][\"message\"][\"content\"])\n        \n        # Add the assistant's response to the message history\n        state[\"messages\"].append({\"role\": \"assistant\", \"content\": answers[0] if answers else \"I couldn't find an answer.\"})\n        \n        return {\"answers\": answers, \"messages\": state[\"messages\"], \"history_summary\": state.get(\"history_summary\", \"\")}\n\n    def summarize(state):\n        summary = requests.post(\n            \"https://api.groq.com/openai/v1/chat/completions\",\n            headers={\"Authorization\": f\"Bearer {GROQ_API_KEY}\"},\n            json={\n                \"model\": GROQ_MODEL,\n                \"messages\": [{\n                    \"role\": \"system\",\n                    \"content\": (\n                        \"You are a helpful support assistant. Your job is to carefully read multiple assistant answers to a user's question \"\n                        \"and combine them into a single, clear, helpful response. The user is not technical, so avoid using any programming terms or code. \"\n                        \"Explain the issue and the solution in plain, language that's easy to understand.\"\n                    )\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": (\n                        f\"The user asked the following question:\\n{state['question']}\\n\\n\"\n                        \"Here are several assistant answers that may help explain the problem:\\n\\n\"\n                        + \"\\n\".join(state[\"answers\"]) +\n                        \"\\n\\nNow, please combine all of this into one simple, clear, and user-friendly explanation of what might be causing the issue and how to fix it.\"\n                    )\n                }]\n            }\n        ).json()\n        \n        final_answer = summary[\"choices\"][0][\"message\"][\"content\"]\n        \n        # We already added an assistant message in generate_answers, but we can update it with the final answer\n        if state[\"messages\"] and state[\"messages\"][-1][\"role\"] == \"assistant\":\n            state[\"messages\"][-1][\"content\"] = final_answer\n        \n        return {\n            \"final_answer\": final_answer, \n            \"messages\": state[\"messages\"],\n            \"history_summary\": state.get(\"history_summary\", \"\")\n        }\n\n    def generate_answers(state):\n        # First manage conversation history\n        state = manage_conversation_history(state)\n        \n        # Combine original context with additional context\n        combined_context = state[\"context\"]\n        if \"additional_context\" in state and state[\"additional_context\"]:\n            combined_context = combined_context + state[\"additional_context\"]\n        \n        # Create messages with context including conversation history\n        system_content = f\"Relevant code: {combined_context}\"\n        \n        # Add conversation history summary if it exists\n        if state.get(\"history_summary\"):\n            system_content = f\"Previous conversation summary: {state['history_summary']}\\n\\n{system_content}\"\n        \n        # Combine the most recent messages and system content\n        messages = [{\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in state[\"messages\"]]\n        messages.append({\"role\": \"system\", \"content\": system_content})\n        \n        answers = []\n        for code in combined_context:\n            response = requests.post(\n                \"https://api.groq.com/openai/v1/chat/completions\",\n                headers={\"Authorization\": f\"Bearer {GROQ_API_KEY}\"},\n                json={\n                    \"model\": GROQ_MODEL,\n                    \"messages\": [{\n                        \"role\": \"system\",\n                        \"content\": (\n                            \"You are a helpful and friendly support assistant for an internal company software. \"\n                            \"You know how the system works, including its features, permissions, and typical issues users may face. \"\n                            \"Your job is to explain possible reasons for the issue and guide the user to a solution in plain, language. \"\n                            \"If the issue might be related to permissions, login state, or a missing step, gently suggest those as things to check. \"\n                            \"Only refer to code if it directly explains behavior, and describe it in user-friendly language.\\n\\n\"\n                            \"Your answers should be formatted clearly with bullet points or steps if needed.\\n\"\n                        )\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": (\n                            f\"A user is facing an issue with the software.\\n\\n\"\n                            f\"Previous conversation context: {state.get('history_summary', 'No prior context')}\\n\\n\"\n                            f\"Here is part of the codebase that may relate to the issue:\\n{code}\\n\\n\"\n                            f\"Recent messages: {state['messages'][-MAX_RECENT_MESSAGES:]}\\n\\n\"\n                            f\"User's Question: {state['question']}\\n\\n\"\n                            \"Please provide a clear, non-technical explanation and steps they can follow to solve the issue.\"\n                        )\n                    }]\n                }\n            ).json()\n            answers.append(response[\"choices\"][0][\"message\"][\"content\"])\n        \n        # Add the assistant's response to the message history\n        state[\"messages\"].append({\"role\": \"assistant\", \"content\": answers[0] if answers else \"I couldn't find an answer.\"})\n        \n        return {\"answers\": answers, \"messages\": state[\"messages\"], \"history_summary\": state.get(\"history_summary\", \"\")}\n    \n    builder.add_node(\"preprocess\", preprocess_with_llama)\n    builder.add_node(\"retrieve\", retrieve)\n    builder.add_node(\"rank_retrieve\", rank_and_filter_retrievals)\n    builder.add_node(\"identify_missing\", identify_missing_functions)\n    builder.add_node(\"retrieve_missing\", retrieve_missing_functions)\n    builder.add_node(\"generate\", generate_answers)\n    builder.add_node(\"summarize\", summarize)\n    \n    builder.set_entry_point(\"preprocess\")\n    builder.add_edge(\"preprocess\", \"retrieve\")\n    builder.add_edge(\"retrieve\", \"rank_retrieve\")\n    builder.add_edge(\"rank_retrieve\", \"identify_missing\")\n    builder.add_edge(\"identify_missing\", \"retrieve_missing\")\n    builder.add_edge(\"retrieve_missing\", \"generate\")\n    builder.add_edge(\"generate\", \"summarize\")\n    builder.add_edge(\"summarize\", END)\n    \n    return builder.compile()","metadata":{"id":"ll2RKvElnwyb","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:06:31.585268Z","iopub.execute_input":"2025-05-19T11:06:31.585563Z","iopub.status.idle":"2025-05-19T11:06:31.643433Z","shell.execute_reply.started":"2025-05-19T11:06:31.585538Z","shell.execute_reply":"2025-05-19T11:06:31.642634Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Generating Response","metadata":{"id":"kWfd3I5rqQj8"}},{"cell_type":"code","source":"workflow = create_workflow()\nconfig = {\"configurable\": {\"thread_id\": \"thread_1\"}}\nquestion = input(\"\\nTell me your issue: \")\n\nresult = workflow.invoke(\n                {\"question\": question, \"messages\": []}, \n                 config=config\n            )\nprint(\"\\nFINAL ANSWER:\", result[\"final_answer\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:06:31.644528Z","iopub.execute_input":"2025-05-19T11:06:31.644793Z","iopub.status.idle":"2025-05-19T11:06:53.701717Z","shell.execute_reply.started":"2025-05-19T11:06:31.644770Z","shell.execute_reply":"2025-05-19T11:06:53.700951Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"\nTell me your issue:  table meh column add nhi ho raha\n"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c4995e645f54a3abfa5291b7ae0f96b"}},"metadata":{}},{"name":"stdout","text":"\nFINAL ANSWER: I'd be happy to help!\n\nIt seems like you're trying to add a new column to a table, but it's not working. Don't worry, we can figure this out!\n\nHere are a few possible reasons why you're facing this issue:\n\n* You might not have the necessary permissions to make changes to the table. \n* The table might be locked or currently in use by someone else or a system process.\n* There could be a technical issue on the server-side that's preventing the change from happening.\n\nTo resolve this issue, try the following steps:\n\n* Check if you have the required permissions to make changes to the table. If you're not sure, you can contact your system administrator or the person who manages the database.\n* Try closing and reopening the database or system to ensure that the table is not locked.\n* If the issue persists, reach out to your system administrator or technical support for assistance. They can help identify and resolve any server-side issues.\n\nRemember, if you're still having trouble, don't hesitate to reach out for further assistance. We're here to help!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from IPython.display import Image, display\n\ndisplay(Image(workflow.get_graph().draw_mermaid_png()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:10:53.311226Z","iopub.execute_input":"2025-05-19T11:10:53.311772Z","iopub.status.idle":"2025-05-19T11:10:54.103577Z","shell.execute_reply.started":"2025-05-19T11:10:53.311750Z","shell.execute_reply":"2025-05-19T11:10:54.102879Z"}},"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":"<IPython.core.display.Image object>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
